{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "data_path = \"data-001.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data-001.jsonl', 'validation.ipynb', '.gitignore', '.env', 'main.py']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 10\n",
      "First example:\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Solve the ODE for SIR model'}, {'role': 'assistant', 'content': 'Here is the ode solver for the SIR model, ```using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,OptimizationOptimisers, Random, Plots #u = [s(t), I(t), R(t)] function trueSirModel!(du, u, p, t) beta, gamma, N = p du[1] = -(beta * u[1] * u[2]) / N du[2] = ((beta * u[1] * u[2]) / N) - (gamma * u[2]) du[3] = gamma * u[2] end # Boundary conditions N = 1000 i0 = 1 r0 = 0 s0 = (N - i0 - r0) u0 = [s0, i0, r0] # constants beta = 0.3 gamma = 0.1 p = (beta, gamma, N)# time duration tspan = (0.0, 160.0) datasize = 160 tsteps = range(tspan[1], tspan[2]; length=datasize) # Solving the ODE solution trueOdeProblem = ODEProblem(trueSirModel!, u0, tspan, p) trueOdeData = Array(solve(trueOdeProblem, Tsit5(), saveat=tsteps))```'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Implement the Neural ODE method for solving the SIR model '}, {'role': 'assistant', 'content': 'Here is the neural ode implementation for the SIR model, ```using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL, OptimizationOptimisers, Random, Plots #u = [s(t), I(t), R(t)] function trueSirModel!(du, u, p, t) beta, gamma, N = p du[1] = -(beta * u[1] * u[2]) / N du[2] = ((beta * u[1] * u[2]) / N) - (gamma * u[2]) du[3] = gamma * u[2] end # Boundary conditions N = 1000 i0 = 1 r0 = 0 s0 = (N - i0 - r0) u0 = [s0, i0, r0] # constants beta = 0.3 gamma = 0.1 p = (beta, gamma, N) # time duration tspan = (0.0, 160.0) datasize = 160 tsteps = range(tspan[1], tspan[2]; length=datasize) # Solving the ODE solution trueOdeProblem = ODEProblem(trueSirModel!, u0, tspan, p) trueOdeData = Array(solve(trueOdeProblem, Tsit5(), saveat=tsteps)) # Defining the Nueral Network rng = Random.default_rng() # After multiple iteraations, the layer with 3x150 fit the true data very well. sirNN = Lux.Chain(Lux.Dense(3, 150, tanh), Lux.Dense(150, 150, tanh), Lux.Dense(150, 3)) p, st = Lux.setup(rng, sirNN) sirNNOde = NeuralODE(sirNN, tspan, Tsit5(), saveat=tsteps) # prediciton function that is determined for every iteration function prediction(p) Array(sirNNOde(u0, p, st)[1]) end # Loss represents the difference between the original data and the predicted output function loss(p) pred = prediction(p) loss = sum(abs2, trueOdeData .- pred) return loss, pred end # A Callback function to plot the output of the true dat and predicted output for suspectible, infected and recvered data points callback = function (p, l, pred; doplot=true) println(l) if doplot plt = scatter(tsteps, trueOdeData[1, :]; label=\"true suspectible\") scatter!(plt, tsteps, pred[1, :]; label=\"prediction suspectible\") iplt = scatter(tsteps, trueOdeData[2, :]; label=\"true infected\") scatter!(iplt, tsteps, pred[2, :]; label=\"prediction infected\") rplt = scatter(tsteps, trueOdeData[3, :]; label=\"true recovered\") scatter!(rplt, tsteps, pred[3, :]; label=\"prediction recovered\") display(plot(plt, iplt, rplt)) end return false end # Defining optimization techniques pinit = ComponentArray(p) adtype = Optimization.AutoZygote() optimizeFunction = Optimization.OptimizationFunction((x, p) -> loss(x), adtype) # Defining the problem to be optimize neuralProblem = Optimization.OptimizationProblem(optimizeFunction, pinit) # NN solver that iterates over 3000 using ADAM optimizer result = Optimization.solve(neuralProblem, Optimisers.Adam(0.001); callback=callback, maxiters=3000)```'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Similarly provide the UDE implementation for the Lotka Voltera eqautions '}, {'role': 'assistant', 'content': 'Here is the UDE implemenation for Lotka Voltera, ```using OrdinaryDiffEq, ModelingToolkit, SciMLSensitivity, DataDrivenSparse, Optimization, OptimizationOptimisers, OptimizationOptimJL, LinearAlgebra, Statistics, ComponentArrays, Lux, Zygote, Plots, StableRNGs; gr(); rng=StableRNG(1111); function lotka!(du,u,p,t) α,β,γ,δ=p; du[1]=α*u[1]-β*u[2]*u[1]; du[2]=γ*u[1]*u[2]-δ*u[2]; end; tspan=(0.0,15.0); u0=5.0f0*rand(rng,2); p_=[1.3,0.9,0.8,1.8]; prob=ODEProblem(lotka!,u0,tspan,p_); solution=solve(prob,Vern7(),abstol=1e-12,reltol=1e-12,saveat=0.25); X=Array(solution); t=solution.t; x̄=mean(X,dims=2); noise_magnitude=5e-3; Xₙ=X.+(noise_magnitude*x̄).*randn(rng,eltype(X),size(X)); plot(solution,alpha=0.75,color=:black,label=[\"True Data\" nothing]); scatter!(t,transpose(Xₙ),color=:red,label=[\"Noisy Data\" nothing]); rbf(x)=exp.(-(x.^2)); const U=Lux.Chain(Lux.Dense(2,5,rbf),Lux.Dense(5,5,rbf),Lux.Dense(5,5,rbf),Lux.Dense(5,2)); p,st=Lux.setup(rng,U); const _st=st; function ude_dynamics!(du,u,p,t,p_true) û=U(u,p,_st)[1]; du[1]=p_true[1]*u[1]+û[1]; du[2]=-p_true[4]*u[2]+û[2]; end; nn_dynamics!(du,u,p,t)=ude_dynamics!(du,u,p,t,p_); prob_nn=ODEProblem(nn_dynamics!,Xₙ[:,1],tspan,p); function predict(θ,X=Xₙ[:,1],T=t) _prob=remake(prob_nn,u0=X,tspan=(T[1],T[end]),p=θ); Array(solve(_prob,Vern7(),saveat=T,abstol=1e-6,reltol=1e-6,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))); end; function loss(θ) X̂=predict(θ); mean(abs2,Xₙ.-X̂); end; losses=Float64[]; callback=function(p,l) push!(losses,l); if length(losses)%50==0 println(\"Current loss after $(length(losses)) iterations: $(losses[end])\"); end; return false; end; adtype=Optimization.AutoZygote(); optf=Optimization.OptimizationFunction((x,p)->loss(x),adtype); optprob=Optimization.OptimizationProblem(optf,ComponentVector{Float64}(p)); res1=Optimization.solve(optprob,ADAM(),callback=callback,maxiters=5000); println(\"Training loss after $(length(losses)) iterations: $(losses[end])\"); optprob2=Optimization.OptimizationProblem(optf,res1.u); res2=Optimization.solve(optprob2,Optim.LBFGS(),callback=callback,maxiters=1000); println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\"); p_trained=res2.u; pl_losses=plot(1:5000,losses[1:5000],yaxis=:log10,xaxis=:log10,xlabel=\"Iterations\",ylabel=\"Loss\",label=\"ADAM\",color=:blue); plot!(5001:length(losses),losses[5001:end],yaxis=:log10,xaxis=:log10,xlabel=\"Iterations\",ylabel=\"Loss\",label=\"BFGS\",color=:red); ts=first(solution.t):(mean(diff(solution.t))/2):last(solution.t); X̂=predict(p_trained,Xₙ[:,1],ts); pl_trajectory=plot(ts,transpose(X̂),xlabel=\"t\",ylabel=\"x(t),y(t)\",color=:red,label=[\"UDE Approximation\" nothing]); scatter!(solution.t,transpose(Xₙ),color=:black,label=[\"Measurements\" nothing]); Ȳ=[-p_[2]*(X̂[1,:].*X̂[2,:])\\';p_[3]*(X̂[1,:].*X̂[2,:])\\']; Ŷ=U(X̂,p_trained,st)[1]; pl_reconstruction=plot(ts,transpose(Ŷ),xlabel=\"t\",ylabel=\"U(x,y)\",color=:red,label=[\"UDE Approximation\" nothing]); plot!(ts,transpose(Ȳ),color=:black,label=[\"True Interaction\" nothing]); pl_reconstruction_error=plot(ts,norm.(eachcol(Ȳ-Ŷ)),yaxis=:log,xlabel=\"t\",ylabel=\"L2-Error\",label=nothing,color=:red); pl_missing=plot(pl_reconstruction,pl_reconstruction_error,layout=(2,1)); pl_overall=plot(pl_trajectory,pl_missing);```'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Provide the Neural ODE implementation for modelling the Black hole dynamics'}, {'role': 'assistant', 'content': '# SciML Tools using OrdinaryDiffEq, ModelingToolkit, DataDrivenDiffEq, SciMLSensitivity, DataDrivenSparse using Optimization, OptimizationOptimisers, OptimizationOptimJL # Standard Libraries using LinearAlgebra, Statistics # External Libraries using ComponentArrays, Lux, Zygote, Plots, StableRNGs, DataFrames, CSV, LineSearches gr() # Set a random seed for reproducible behaviour rng = StableRNG(1111) mass_ratio = 0.0 # test particle u0 = Float64[pi, 0.0] # initial conditions datasize = 250 tspan = (0.0f0, 6.0f4) # timespace for GW waveform tsteps = range(tspan[1], tspan[2], length = datasize) # time at each timestep dt_data = tsteps[2] - tsteps[1] dt = 100.0 model_params = [100.0, 1.0, 0.5]; # p, M, e prob = ODEProblem(RelativisticOrbitModel, u0, tspan, model_params) soln = Array(solve(prob, RK4(), saveat = tsteps, dt = dt, adaptive = false)) waveform = compute_waveform(dt_data, soln, mass_ratio, model_params)[1] plt = plot(tsteps, waveform, markershape = :circle, markeralpha = 0.25, linewidth = 2, alpha = 0.5, label = \"waveform data\", xlabel = \"Time\", ylabel = \"Waveform\") NN = Lux.Chain((x) -> cos.(x), Lux.Dense(1, 32, cos), Lux.Dense(32, 32, cos), Lux.Dense(32, 2)) p, st = Lux.setup(rng, NN) NN_params = ComponentArray{Float64}(p) function ODE_model(u, NN_params, t) du = AbstractNNOrbitModel(u, model_params, t, NN = NN, NN_params = NN_params) return du end prob_nn = ODEProblem(ODE_model, u0, tspan, NN_params) soln_nn = Array(solve( prob_nn, RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive = false)) waveform_nn = compute_waveform(dt_data, soln_nn, mass_ratio, model_params)[1] plot!(plt, tsteps, waveform_nn, markershape = :circle, markeralpha = 0.25, linewidth = 2, alpha = 0.5, label = \"waveform NN\") display(plt) function loss(NN_params) first_obs_to_use_for_training = 1 last_obs_to_use_for_training = length(waveform) obs_to_use_for_training = first_obs_to_use_for_training:last_obs_to_use_for_training pred = Array(solve( prob_nn, RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive = false)) pred_waveform = compute_waveform(dt_data, pred, mass_ratio, model_params)[1] loss = ( sum(abs2, view(waveform,obs_to_use_for_training) .- view(pred_waveform,obs_to_use_for_training) ) ) return loss end losses = [] callback(state, l; doplot = true) = begin push!(losses, l) #= Disable plotting as it trains since in docs display(l) waveform = compute_waveform(dt_data, soln, mass_ratio, model_params)[1] # plot current prediction against data plt = plot(tsteps, waveform, markershape=:circle, markeralpha = 0.25, linewidth = 2, alpha = 0.5, label=\"wform data (h22)\", legend=:topleft) plot!(plt, tsteps, pred_waveform, markershape=:circle, markeralpha = 0.25, linewidth = 2, alpha = 0.5, label = \"wform NN\") if doplot display(plot(plt)) end # Tell sciml_train to not halt the optimization. If return true, then # optimization stops. =# return false end NN_params = NN_params .* 0 + Float64(1e-4) * randn(StableRNG(2031), eltype(NN_params), size(NN_params)) adtype = Optimization.AutoZygote() optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype) optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(NN_params)) res1 = Optimization.solve( optprob, OptimizationOptimisers.Adam(0.001f0), callback = callback, maxiters = 100) optprob = Optimization.OptimizationProblem(optf, res1.u) res2 = Optimization.solve( optprob, BFGS(initial_stepnorm = 0.01, linesearch = LineSearches.BackTracking()), callback = callback, maxiters = 20) reference_solution = solve(remake(prob, p = model_params, saveat = tsteps, tspan = tspan), RK4(), dt = dt, adaptive = false) optimized_solution = solve( remake(prob_nn, p = res2.minimizer, saveat = tsteps, tspan = tspan), RK4(), dt = dt, adaptive = false) Newtonian_prob = ODEProblem(NewtonianOrbitModel, u0, tspan, model_params) Newtonian_solution = solve( remake(Newtonian_prob, p = model_params, saveat = tsteps, tspan = tspan), RK4(), dt = dt, adaptive = false) true_orbit = soln2orbit(reference_solution, model_params) pred_orbit = soln2orbit(optimized_solution, model_params) Newt_orbit = soln2orbit(Newtonian_solution, model_params) true_waveform = compute_waveform(dt_data, reference_solution, mass_ratio, model_params)[1] pred_waveform = compute_waveform(dt_data, optimized_solution, mass_ratio, model_params)[1] Newt_waveform = compute_waveform(dt_data, Newtonian_solution, mass_ratio, model_params)[1] true_orbit = soln2orbit(reference_solution, model_params) pred_orbit = soln2orbit(optimized_solution, model_params) Newt_orbit = soln2orbit(Newtonian_solution, model_params) plt = plot(true_orbit[1, :], true_orbit[2, :], linewidth = 2, label = \"truth\") plot!(plt, pred_orbit[1, :], pred_orbit[2, :], linestyle = :dash, linewidth = 2, label = \"prediction\") plot!(plt, Newt_orbit[1, :], Newt_orbit[2, :], linewidth = 2, label = \"Newtonian\") plt = plot(tsteps, true_waveform, linewidth = 2, label = \"truth\", xlabel = \"Time\", ylabel = \"Waveform\") plot!(plt, tsteps, pred_waveform, linestyle = :dash, linewidth = 2, label = \"prediction\") plot!(plt, tsteps, Newt_waveform, linewidth = 2, label = \"Newtonian\") factor = 5 extended_tspan = (tspan[1], factor * tspan[2]) extended_tsteps = range(tspan[1], factor * tspan[2], length = factor * datasize) reference_solution = solve( remake(prob, p = model_params, saveat = extended_tsteps, tspan = extended_tspan), RK4(), dt = dt, adaptive = false) optimized_solution = solve( remake(prob_nn, p = res2.minimizer, saveat = extended_tsteps, tspan = extended_tspan), RK4(), dt = dt, adaptive = false) Newtonian_prob = ODEProblem(NewtonianOrbitModel, u0, tspan, model_params) Newtonian_solution = solve( remake( Newtonian_prob, p = model_params, saveat = extended_tsteps, tspan = extended_tspan), RK4(), dt = dt, adaptive = false) true_orbit = soln2orbit(reference_solution, model_params) pred_orbit = soln2orbit(optimized_solution, model_params) Newt_orbit = soln2orbit(Newtonian_solution, model_params) plt = plot(true_orbit[1, :], true_orbit[2, :], linewidth = 2, label = \"truth\") plot!(plt, pred_orbit[1, :], pred_orbit[2, :], linestyle = :dash, linewidth = 2, label = \"prediction\") plot!(plt, Newt_orbit[1, :], Newt_orbit[2, :], linewidth = 2, label = \"Newtonian\") true_waveform = compute_waveform(dt_data, reference_solution, mass_ratio, model_params)[1] pred_waveform = compute_waveform(dt_data, optimized_solution, mass_ratio, model_params)[1] Newt_waveform = compute_waveform(dt_data, Newtonian_solution, mass_ratio, model_params)[1] plt = plot(extended_tsteps, true_waveform, linewidth = 2, label = \"truth\", xlabel = \"Time\", ylabel = \"Waveform\") plot!(plt, extended_tsteps, pred_waveform, linestyle = :dash, linewidth = 2, label = \"prediction\")'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Model the uncertaininity of Uncertainty quantified discovery of chemical reaction using SciML'}, {'role': 'assistant', 'content': 'using Flux, Random, Plots; using DiffEqFlux, DifferentialEquations, LinearAlgebra, Statistics, BSON: @save, @load; Random.seed!(1234); is_restart = false; p_cutoff = 0.0; n_epoch = 60000; n_plot = 100; opt = ADAMW(0.001, (0.9, 0.999), 1f-8); datasize = 75; tstep = 0.4; n_exp_train = 100; n_exp_test = 10; n_exp = n_exp_train + n_exp_test; noise = 0; ns = 5; nr = 4; k = Float32[0.1, 0.2, 0.13, 0.3]; alg = Tsit5(); atol = 1e-5; rtol = 1e-2; maxiters = 1000000; lb = 1f-5; ub = 1f1; function trueODEfunc(dydt, y, k, t); dydt[1] = -2 * k[1] * y[1]^2 - k[2] * y[1]; dydt[2] = k[1] * y[1]^2 - k[4] * y[2] * y[4]; dydt[3] = k[2] * y[1] - k[3] * y[3]; dydt[4] = k[3] * y[3] - k[4] * y[2] * y[4]; dydt[5] = k[4] * y[2] * y[4]; end; u0_list = rand(Float32, (n_exp, ns)); u0_list[:, 1:2] .+= 0.2; u0_list[:, 3:end] .= 0f0; tspan = Float32[0.0, datasize * tstep]; tsteps = range(tspan[1], tspan[2], length=datasize); ode_data_list = zeros(Float32, (n_exp, ns, datasize)); std_list = []; function max_min(ode_data); return maximum(ode_data, dims=2) .- minimum(ode_data, dims=2) .+ lb; end; for i in 1:n_exp; u0 = u0_list[i, :]; prob_trueode = ODEProblem(trueODEfunc, u0, tspan, k); ode_data = Array(solve(prob_trueode, alg, saveat=tsteps)); ode_data += randn(size(ode_data)) .* ode_data .* noise; ode_data_list[i, :, :] = ode_data; push!(std_list, max_min(ode_data)); end; y_std = maximum(hcat(std_list...), dims=2); u0_list = u0_list\\'; ode_data_list = permutedims(ode_data_list, [2, 3, 1]); data = Flux.DataLoader((u0_list, ode_data_list), batchsize=10); dudt = Chain(Dense(5, 50, tanh), Dense(50, 50, tanh), Dense(50, 5)); n_ode = NeuralODE(dudt, tspan, Tsit5(), saveat=tsteps); function loss_n_ode(u0, y, n_ode); result = n_ode(u0); result = permutedims(result, [1, 3, 2]); return mae(y, result); end; opt_state = Flux.setup(Adam(0.001), n_ode); loss_history = Float32[]; for epoch in 1:1000; Flux.train!(n_ode, data, opt_state) do n_ode, x, y; err = loss_n_ode(x, y, n_ode); ChainRules.ignore_derivatives() do; push!(loss_history, err); end; @show err; return err; end; end; prediction = n_ode(u0_list[:, 1]); plot(prediction[1, :]); plot!(ode_data_list[1, :, 1]); pred_ode_data = zeros(Float64, (n_exp_train, ns, datasize)); extrap_ode_data = zeros(Float64, (n_exp_train, ns, datasize)); for i in 1:n_exp_train; u0_i = u0_list[:, i]; pred = Array(n_ode(u0_i)); pred_ode_data[i, :, :] = pred; u0_n = [pred[1, end], pred[2, end], pred[3, end], pred[4, end], pred[5, end]]; extra_pred = Array(n_ode(u0_n)); extrap_ode_data[i, :, :] = extra_pred; end; datasize = 100; tspan = Float32[0.0, datasize * tstep]; tsteps = range(tspan[1], tspan[2], length=datasize); ode_data_list = zeros(Float32, (n_exp_train, ns, datasize)); for i in 1:n_exp_train; u0 = u0_list[:, i]; prob_trueode = ODEProblem(trueODEfunc, u0, tspan, k); ode_data = Array(solve(prob_trueode, alg, saveat=tsteps)); ode_data_list[i, :, :] = ode_data; end; JLD2.@save \"simp_rxn_nODE_100_truthv4\" ode_data_list;'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Using UDEs model the non newtonian fluids'}, {'role': 'assistant', 'content': 'using Flux, Random, Plots; using DiffEqFlux, DifferentialEquations, LinearAlgebra, Statistics, BSON: @save, @load; Random.seed!(1234); is_restart = false; p_cutoff = 0.0; n_epoch = 60000; n_plot = 100; opt = ADAMW(0.001, (0.9, 0.999), 1f-8); datasize = 75; tstep = 0.4; n_exp_train = 100; n_exp_test = 10; n_exp = n_exp_train + n_exp_test; noise = 0; ns = 5; nr = 4; k = Float32[0.1, 0.2, 0.13, 0.3]; alg = Tsit5(); atol = 1e-5; rtol = 1e-2; maxiters = 1000000; lb = 1f-5; ub = 1f1; function trueODEfunc(dydt, y, k, t); dydt[1] = -2 * k[1] * y[1]^2 - k[2] * y[1]; dydt[2] = k[1] * y[1]^2 - k[4] * y[2] * y[4]; dydt[3] = k[2] * y[1] - k[3] * y[3]; dydt[4] = k[3] * y[3] - k[4] * y[2] * y[4]; dydt[5] = k[4] * y[2] * y[4]; end; u0_list = rand(Float32, (n_exp, ns)); u0_list[:, 1:2] .+= 0.2; u0_list[:, 3:end] .= 0f0; tspan = Float32[0.0, datasize * tstep]; tsteps = range(tspan[1], tspan[2], length=datasize); ode_data_list = zeros(Float32, (n_exp, ns, datasize)); std_list = []; function max_min(ode_data); return maximum(ode_data, dims=2) .- minimum(ode_data, dims=2) .+ lb; end; for i in 1:n_exp; u0 = u0_list[i, :]; prob_trueode = ODEProblem(trueODEfunc, u0, tspan, k); ode_data = Array(solve(prob_trueode, alg, saveat=tsteps)); ode_data += randn(size(ode_data)) .* ode_data .* noise; ode_data_list[i, :, :] = ode_data; push!(std_list, max_min(ode_data)); end; y_std = maximum(hcat(std_list...), dims=2); u0_list = u0_list\\'; ode_data_list = permutedims(ode_data_list, [2, 3, 1]); data = Flux.DataLoader((u0_list, ode_data_list), batchsize=10); dudt = Chain(Dense(5, 50, tanh), Dense(50, 50, tanh), Dense(50, 5)); n_ode = NeuralODE(dudt, tspan, Tsit5(), saveat=tsteps); function loss_n_ode(u0, y, n_ode); result = n_ode(u0); result = permutedims(result, [1, 3, 2]); return mae(y, result); end; opt_state = Flux.setup(Adam(0.001), n_ode); loss_history = Float32[]; for epoch in 1:1000; Flux.train!(n_ode, data, opt_state) do n_ode, x, y; err = loss_n_ode(x, y, n_ode); ChainRules.ignore_derivatives() do; push!(loss_history, err); end; @show err; return err; end; end; prediction = n_ode(u0_list[:, 1]); plot(prediction[1, :]); plot!(ode_data_list[1, :, 1]); pred_ode_data = zeros(Float64, (n_exp_train, ns, datasize)); extrap_ode_data = zeros(Float64, (n_exp_train, ns, datasize)); for i in 1:n_exp_train; u0_i = u0_list[:, i]; pred = Array(n_ode(u0_i)); pred_ode_data[i, :, :] = pred; u0_n = [pred[1, end], pred[2, end], pred[3, end], pred[4, end], pred[5, end]]; extra_pred = Array(n_ode(u0_n)); extrap_ode_data[i, :, :] = extra_pred; end; datasize = 100; tspan = Float32[0.0, datasize * tstep]; tsteps = range(tspan[1], tspan[2], length=datasize); ode_data_list = zeros(Float32, (n_exp_train, ns, datasize)); for i in 1:n_exp_train; u0 = u0_list[:, i]; prob_trueode = ODEProblem(trueODEfunc, u0, tspan, k); ode_data = Array(solve(prob_trueode, alg, saveat=tsteps)); ode_data_list[i, :, :] = ode_data; end; JLD2.@save \"simp_rxn_nODE_100_truthv4\" ode_data_list;'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Give the UDE implementation of Hamilton Jacobi Bellman Equation'}, {'role': 'assistant', 'content': 'cd(@__DIR__); using Pkg; Pkg.activate(\".\"); Pkg.instantiate(); using Flux, StochasticDiffEq, LinearAlgebra, Statistics, Test, NeuralNetDiffEq; using Random; Random.seed!(0); d = 100; x0 = fill(0.0f0,d); tspan = (0.0f0, 1.0f0); m = 100; λ = 1.0f0; g(X) = log(0.5f0 + 0.5f0*sum(X.^2)); f(X,u,σᵀ∇u,p,t) = -λ*sum(σᵀ∇u.^2); μ(X,p,t) = zero(X); σ(X,p,t) = Diagonal(sqrt(2.0f0)*ones(Float32,d)); prob = TerminalPDEProblem(g, f, μ, σ, x0, tspan); hls = 10 + d; opt = Flux.ADAM(0.03); u0 = Flux.Chain(Dense(d,hls,relu), Dense(hls,hls,relu), Dense(hls,1)); σᵀ∇u = Flux.Chain(Dense(d+1,hls,relu), Dense(hls,hls,relu), Dense(hls,hls,relu), Dense(hls,d)); pdealg = NNPDENS(u0, σᵀ∇u, opt=opt); @time ans = solve(prob, pdealg, verbose=true, maxiters=500, trajectories=m, alg=LambaEM(), pabstol = 1f-2, reltol=1e-4, abstol=1e-4); T = tspan[2]; MC = 10^5; W() = randn(d,1); u_analytical(x, t) = -(1/λ)*log(mean(exp(-λ*g(x .+ sqrt(2.0)*abs.(T-t).*W())) for _ = 1:MC)); analytical_ans = u_analytical(x0, tspan[1]); error_l2 = sqrt((ans - analytical_ans)^2/ans^2); println(\"Hamilton Jacobi Bellman Equation\"); println(\"error_l2 = \", error_l2, \"n\"); @test error_l2 < 0.2'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Give the UDE implementation of Hamilton Jacobi Bellman Equation'}, {'role': 'assistant', 'content': 'cd(@__DIR__); using Pkg; Pkg.activate(\".\"); Pkg.instantiate(); using Flux, StochasticDiffEq, LinearAlgebra, Statistics, Test, NeuralNetDiffEq; using Random; Random.seed!(0); d = 100; x0 = fill(0.0f0,d); tspan = (0.0f0, 1.0f0); m = 100; λ = 1.0f0; g(X) = log(0.5f0 + 0.5f0*sum(X.^2)); f(X,u,σᵀ∇u,p,t) = -λ*sum(σᵀ∇u.^2); μ(X,p,t) = zero(X); σ(X,p,t) = Diagonal(sqrt(2.0f0)*ones(Float32,d)); prob = TerminalPDEProblem(g, f, μ, σ, x0, tspan); hls = 10 + d; opt = Flux.ADAM(0.03); u0 = Flux.Chain(Dense(d,hls,relu), Dense(hls,hls,relu), Dense(hls,1)); σᵀ∇u = Flux.Chain(Dense(d+1,hls,relu), Dense(hls,hls,relu), Dense(hls,hls,relu), Dense(hls,d)); pdealg = NNPDENS(u0, σᵀ∇u, opt=opt); @time ans = solve(prob, pdealg, verbose=true, maxiters=500, trajectories=m, alg=LambaEM(), pabstol = 1f-2, reltol=1e-4, abstol=1e-4); T = tspan[2]; MC = 10^5; W() = randn(d,1); u_analytical(x, t) = -(1/λ)*log(mean(exp(-λ*g(x .+ sqrt(2.0)*abs.(T-t).*W())) for _ = 1:MC)); analytical_ans = u_analytical(x0, tspan[1]); error_l2 = sqrt((ans - analytical_ans)^2/ans^2); println(\"Hamilton Jacobi Bellman Equation\"); println(\"error_l2 = \", error_l2, \"n\"); @test error_l2 < 0.2'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Solve the simple PDE solver for Schrodinger equation'}, {'role': 'assistant', 'content': 'using OrdinaryDiffEq, ModelingToolkit, MethodOfLines, DomainSets; @parameters t, x; @variables psi(..); Dt = Differential(t); Dxx = Differential(x)^2; xmin = 0; xmax = 1; V(x) = 0.0; eq = [im * Dt(psi(t, x)) ~ Dxx(psi(t, x)) + V(x) * psi(t, x)]; psi0 = x -> sin(2pi * x); bcs = [psi(0, x) ~ psi0(x), psi(t, xmin) ~ 0, psi(t, xmax) ~ 0]; domains = [t ∈ Interval(0, 1), x ∈ Interval(xmin, xmax)]; @named sys = PDESystem(eq, bcs, domains, [t, x], [psi(t, x)]); disc = MOLFiniteDifference([x => 100], t); prob = discretize(sys, disc); sol = solve(prob, TRBDF2(), saveat=0.01); discx = sol[x]; disct = sol[t]; sol[psi(t, x)]; using Plots; discPsi = sol[psi(t, x)]; anim = @animate for i in 1:length(disct); u = discPsi[i, :]; plot(discx, [real.(u), imag.(u)], ylim=(-1.5, 1.5), title=\"t = $(disct[i])\", xlabel=\"x\", ylabel=\"psi(t,x)\", label=[\"re(psi)\" \"im(psi)\"], legend=:topleft); end; gif(anim, \"schroedinger.gif\", fps=10)'}]\n",
      "[{'role': 'system', 'content': 'SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems.'}, {'role': 'user', 'content': 'Solve the PDE equation system for 1D wave equation using Neural ODE'}, {'role': 'assistant', 'content': 'using NeuralPDE, Lux, Optimization, OptimizationOptimJL; using ModelingToolkit: Interval; @parameters t, x; @variables u(..); @derivatives Dxx\\'\\' ~ x; @derivatives Dtt\\'\\' ~ t; @derivatives Dt\\' ~ t; C = 1; eq = Dtt(u(t, x)) ~ C^2 * Dxx(u(t, x)); bcs = [u(t, 0) ~ 0.0, u(t, 1) ~ 0.0, u(0, x) ~ x * (1.0 - x), Dt(u(0, x)) ~ 0.0]; domains = [t ∈ Interval(0.0, 1.0), x ∈ Interval(0.0, 1.0)]; dx = 0.1; chain = Lux.Chain(Dense(2, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1)); discretization = PhysicsInformedNN(chain, GridTraining(dx)); @named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x)]); prob = discretize(pde_system, discretization); callback = function (_, l); println(\"Loss is: $l\"); return false; end; opt = OptimizationOptimJL.BFGS(); res = Optimization.solve(prob, opt; callback=callback, maxiters=1500); phi = discretization.phi; using Plots; ts, xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]; function analytic_sol_func(t, x); sum([(8 / (k^3 * pi^3)) * sin(k * pi * x) * cos(C * k * pi * t) for k in 1:2:50000]); end; u_predict = reshape([first(phi([t, x], res.u)) for t in ts for x in xs], (length(ts), length(xs))); u_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs], (length(ts), length(xs))); p1 = plot(ts, xs, u_real, linetype=:contourf, title=\"analytic\"); p2 = plot(ts, xs, u_predict, linetype=:contourf, title=\"predict\"); plot(p1, p2)'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset:\n",
    "    print(message[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 335, 2028\n",
      "mean / median: 886.2, 680.5\n",
      "p5 / p95: 391.7, 1308.8999999999996\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 292, 1982\n",
      "mean / median: 839.2, 631.5\n",
      "p5 / p95: 346.0, 1259.2999999999997\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~8862 tokens that will be charged for during training\n",
      "By default, you'll train for 10 epochs on this dataset\n",
      "By default, you'll be charged for ~88620 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
