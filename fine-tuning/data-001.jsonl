{"messages": [{"role": "system", "content": "SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems."}, {"role": "user", "content": "Solve the ODE for SIR model"}, {"role": "assistant", "content": "Here is the ode solver for the SIR model, ```using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,OptimizationOptimisers, Random, Plots #u = [s(t), I(t), R(t)] function trueSirModel!(du, u, p, t) beta, gamma, N = p du[1] = -(beta * u[1] * u[2]) / N du[2] = ((beta * u[1] * u[2]) / N) - (gamma * u[2]) du[3] = gamma * u[2] end # Boundary conditions N = 1000 i0 = 1 r0 = 0 s0 = (N - i0 - r0) u0 = [s0, i0, r0] # constants beta = 0.3 gamma = 0.1 p = (beta, gamma, N)# time duration tspan = (0.0, 160.0) datasize = 160 tsteps = range(tspan[1], tspan[2]; length=datasize) # Solving the ODE solution trueOdeProblem = ODEProblem(trueSirModel!, u0, tspan, p) trueOdeData = Array(solve(trueOdeProblem, Tsit5(), saveat=tsteps))```"}]}
{"messages": [{"role": "system", "content": "SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems."}, {"role": "user", "content": "Implement the Neural ODE method for solving the SIR model "}, {"role": "assistant", "content": "Here is the neural ode implementation for the SIR model, ```using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL, OptimizationOptimisers, Random, Plots #u = [s(t), I(t), R(t)] function trueSirModel!(du, u, p, t) beta, gamma, N = p du[1] = -(beta * u[1] * u[2]) / N du[2] = ((beta * u[1] * u[2]) / N) - (gamma * u[2]) du[3] = gamma * u[2] end # Boundary conditions N = 1000 i0 = 1 r0 = 0 s0 = (N - i0 - r0) u0 = [s0, i0, r0] # constants beta = 0.3 gamma = 0.1 p = (beta, gamma, N) # time duration tspan = (0.0, 160.0) datasize = 160 tsteps = range(tspan[1], tspan[2]; length=datasize) # Solving the ODE solution trueOdeProblem = ODEProblem(trueSirModel!, u0, tspan, p) trueOdeData = Array(solve(trueOdeProblem, Tsit5(), saveat=tsteps)) # Defining the Nueral Network rng = Random.default_rng() # After multiple iteraations, the layer with 3x150 fit the true data very well. sirNN = Lux.Chain(Lux.Dense(3, 150, tanh), Lux.Dense(150, 150, tanh), Lux.Dense(150, 3)) p, st = Lux.setup(rng, sirNN) sirNNOde = NeuralODE(sirNN, tspan, Tsit5(), saveat=tsteps) # prediciton function that is determined for every iteration function prediction(p) Array(sirNNOde(u0, p, st)[1]) end # Loss represents the difference between the original data and the predicted output function loss(p) pred = prediction(p) loss = sum(abs2, trueOdeData .- pred) return loss, pred end # A Callback function to plot the output of the true dat and predicted output for suspectible, infected and recvered data points callback = function (p, l, pred; doplot=true) println(l) if doplot plt = scatter(tsteps, trueOdeData[1, :]; label=\"true suspectible\") scatter!(plt, tsteps, pred[1, :]; label=\"prediction suspectible\") iplt = scatter(tsteps, trueOdeData[2, :]; label=\"true infected\") scatter!(iplt, tsteps, pred[2, :]; label=\"prediction infected\") rplt = scatter(tsteps, trueOdeData[3, :]; label=\"true recovered\") scatter!(rplt, tsteps, pred[3, :]; label=\"prediction recovered\") display(plot(plt, iplt, rplt)) end return false end # Defining optimization techniques pinit = ComponentArray(p) adtype = Optimization.AutoZygote() optimizeFunction = Optimization.OptimizationFunction((x, p) -> loss(x), adtype) # Defining the problem to be optimize neuralProblem = Optimization.OptimizationProblem(optimizeFunction, pinit) # NN solver that iterates over 3000 using ADAM optimizer result = Optimization.solve(neuralProblem, Optimisers.Adam(0.001); callback=callback, maxiters=3000)```"}]}
{"messages": [{"role": "system", "content": "SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems."}, {"role": "user", "content": "Similarly provide the UDE implementation for the Lotka Voltera eqautions "}, {"role": "assistant", "content": "Here is the UDE implemenation for Lotka Voltera, ```using OrdinaryDiffEq, ModelingToolkit, SciMLSensitivity, DataDrivenSparse, Optimization, OptimizationOptimisers, OptimizationOptimJL, LinearAlgebra, Statistics, ComponentArrays, Lux, Zygote, Plots, StableRNGs; gr(); rng=StableRNG(1111); function lotka!(du,u,p,t) α,β,γ,δ=p; du[1]=α*u[1]-β*u[2]*u[1]; du[2]=γ*u[1]*u[2]-δ*u[2]; end; tspan=(0.0,15.0); u0=5.0f0*rand(rng,2); p_=[1.3,0.9,0.8,1.8]; prob=ODEProblem(lotka!,u0,tspan,p_); solution=solve(prob,Vern7(),abstol=1e-12,reltol=1e-12,saveat=0.25); X=Array(solution); t=solution.t; x̄=mean(X,dims=2); noise_magnitude=5e-3; Xₙ=X.+(noise_magnitude*x̄).*randn(rng,eltype(X),size(X)); plot(solution,alpha=0.75,color=:black,label=[\"True Data\" nothing]); scatter!(t,transpose(Xₙ),color=:red,label=[\"Noisy Data\" nothing]); rbf(x)=exp.(-(x.^2)); const U=Lux.Chain(Lux.Dense(2,5,rbf),Lux.Dense(5,5,rbf),Lux.Dense(5,5,rbf),Lux.Dense(5,2)); p,st=Lux.setup(rng,U); const _st=st; function ude_dynamics!(du,u,p,t,p_true) û=U(u,p,_st)[1]; du[1]=p_true[1]*u[1]+û[1]; du[2]=-p_true[4]*u[2]+û[2]; end; nn_dynamics!(du,u,p,t)=ude_dynamics!(du,u,p,t,p_); prob_nn=ODEProblem(nn_dynamics!,Xₙ[:,1],tspan,p); function predict(θ,X=Xₙ[:,1],T=t) _prob=remake(prob_nn,u0=X,tspan=(T[1],T[end]),p=θ); Array(solve(_prob,Vern7(),saveat=T,abstol=1e-6,reltol=1e-6,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))); end; function loss(θ) X̂=predict(θ); mean(abs2,Xₙ.-X̂); end; losses=Float64[]; callback=function(p,l) push!(losses,l); if length(losses)%50==0 println(\"Current loss after $(length(losses)) iterations: $(losses[end])\"); end; return false; end; adtype=Optimization.AutoZygote(); optf=Optimization.OptimizationFunction((x,p)->loss(x),adtype); optprob=Optimization.OptimizationProblem(optf,ComponentVector{Float64}(p)); res1=Optimization.solve(optprob,ADAM(),callback=callback,maxiters=5000); println(\"Training loss after $(length(losses)) iterations: $(losses[end])\"); optprob2=Optimization.OptimizationProblem(optf,res1.u); res2=Optimization.solve(optprob2,Optim.LBFGS(),callback=callback,maxiters=1000); println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\"); p_trained=res2.u; pl_losses=plot(1:5000,losses[1:5000],yaxis=:log10,xaxis=:log10,xlabel=\"Iterations\",ylabel=\"Loss\",label=\"ADAM\",color=:blue); plot!(5001:length(losses),losses[5001:end],yaxis=:log10,xaxis=:log10,xlabel=\"Iterations\",ylabel=\"Loss\",label=\"BFGS\",color=:red); ts=first(solution.t):(mean(diff(solution.t))/2):last(solution.t); X̂=predict(p_trained,Xₙ[:,1],ts); pl_trajectory=plot(ts,transpose(X̂),xlabel=\"t\",ylabel=\"x(t),y(t)\",color=:red,label=[\"UDE Approximation\" nothing]); scatter!(solution.t,transpose(Xₙ),color=:black,label=[\"Measurements\" nothing]); Ȳ=[-p_[2]*(X̂[1,:].*X̂[2,:])';p_[3]*(X̂[1,:].*X̂[2,:])']; Ŷ=U(X̂,p_trained,st)[1]; pl_reconstruction=plot(ts,transpose(Ŷ),xlabel=\"t\",ylabel=\"U(x,y)\",color=:red,label=[\"UDE Approximation\" nothing]); plot!(ts,transpose(Ȳ),color=:black,label=[\"True Interaction\" nothing]); pl_reconstruction_error=plot(ts,norm.(eachcol(Ȳ-Ŷ)),yaxis=:log,xlabel=\"t\",ylabel=\"L2-Error\",label=nothing,color=:red); pl_missing=plot(pl_reconstruction,pl_reconstruction_error,layout=(2,1)); pl_overall=plot(pl_trajectory,pl_missing);```"}]}
{"messages": [{"role": "system", "content": "SciMLGPT is a chatbot trained to generate Julia code for Scientific machine learning problems."}, {"role": "user", "content": ""}]}

